# Copy to .env and adjust as needed.
#
# Usage:
#   uv run mercury-cli                                    # Ollama (default)
#   uv run mercury-cli --provider anthropic               # Claude
#   uv run mercury-cli --provider openai                  # GPT-4o
#   uv run mercury-cli --provider openrouter              # Any model via OpenRouter
#   uv run mercury-web  --provider anthropic --port 3000  # Web UI with Claude

# Provider: ollama | anthropic | openai | openrouter
PROVIDER=ollama

# Model name (leave empty for provider default)
# Ollama: llama3, glm-4.7-flash:latest, etc.
# Anthropic: claude-sonnet-4-20250514, claude-3-5-haiku-20241022, etc.
# OpenAI: gpt-4o, gpt-4o-mini, etc.
# OpenRouter: anthropic/claude-sonnet-4-20250514, google/gemini-2.0-flash, etc.
MODEL=

# API keys (set the one matching your provider)
# ANTHROPIC_API_KEY=sk-ant-...
# OPENAI_API_KEY=sk-...
# OPENROUTER_API_KEY=sk-or-...
# Or use the generic key (works for any provider):
# API_KEY=...

# Custom API base URL (overrides provider default)
# API_BASE_URL=

# Ollama-specific
OLLAMA_HOST=http://localhost:11434

# Mercury Playground
MERCURY_URL=http://localhost:8080
MERCURY_PLAYGROUND_DIR=~/mercury-playground

# Context window budget (auto-detected per provider if not set)
# Ollama: 8192, Anthropic: 200000, OpenAI: 128000
# CONTEXT_WINDOW=8192
